# Deploy éƒ¨ç½²æ¨¡å—

> æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†åŠ é€Ÿå·¥å…·ï¼Œæ”¯æŒONNXã€TensorRTã€å…±äº«å†…å­˜ç­‰å¤šç§éƒ¨ç½²æ–¹å¼

## ğŸ“‹ ç›®å½•

- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **å¤šæ ¼å¼æ”¯æŒ**: ONNXã€TensorRTã€OpenVINOç­‰æ¨ç†å¼•æ“
- ğŸ’¾ **å…±äº«å†…å­˜**: é«˜æ•ˆçš„è¿›ç¨‹é—´æ•°æ®ä¼ è¾“
- ğŸ”§ **æ¨¡å‹ä¼˜åŒ–**: è‡ªåŠ¨æ¨¡å‹é‡åŒ–å’Œä¼˜åŒ–
- ğŸ“Š **æ€§èƒ½ç›‘æ§**: å®æ—¶æ¨ç†æ€§èƒ½ç»Ÿè®¡
- ğŸ”„ **çƒ­æ›´æ–°**: æ”¯æŒæ¨¡å‹åŠ¨æ€æ›´æ–°
- ğŸ›¡ï¸ **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
- ğŸ“ˆ **å¯æ‰©å±•**: æ”¯æŒè‡ªå®šä¹‰æ¨ç†åç«¯

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from sindre.deploy import onnxruntime_deploy, TenserRT_deploy

# ONNX Runtimeéƒ¨ç½²
onnx_infer = onnxruntime_deploy.OnnxInfer("model.onnx")
result = onnx_infer(input_data)

# TensorRTéƒ¨ç½²
trt_infer = TenserRT_deploy.TRTInfer()
trt_infer.load_model("model.engine")
result = trt_infer(input_data)
```

### å…±äº«å†…å­˜éƒ¨ç½²

```python
from sindre.deploy import python_share_memory

# åˆ›å»ºå…±äº«å†…å­˜æœåŠ¡
server = python_share_memory.SharedMemoryServer("model_server")
server.start()

# å®¢æˆ·ç«¯è¿æ¥
client = python_share_memory.SharedMemoryClient("model_server")
result = client.infer(input_data)
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### ONNX Runtime éƒ¨ç½²

```python
class OnnxInfer:
    """ONNXæ¨¡å‹æ¨ç†ç±»"""
    
    def __init__(self, onnx_path: str, providers: List[Tuple[str, Dict[str, Any]]] = [('CPUExecutionProvider', {})], enable_log: bool = False):
        """
        åˆå§‹åŒ–ONNXæ¨ç†
        
        Args:
            onnx_path: ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„
            providers: æ¨ç†æä¾›è€…åˆ—è¡¨
            enable_log: æ˜¯å¦å¯ç”¨æ—¥å¿—
        """
    
    def __call__(self, inputs: np.ndarray) -> List[np.ndarray]:
        """
        æ‰§è¡Œæ¨¡å‹æ¨ç†
        
        Args:
            inputs: è¾“å…¥æ•°æ®ï¼ˆnumpyæ•°ç»„æˆ–å­—å…¸ï¼‰
            
        Returns:
            List[np.ndarray]: æ¨ç†ç»“æœ
        """
    
    def optimizer(self, save_onnx: str):
        """
        ä¼˜åŒ–å¹¶ç®€åŒ–ONNXæ¨¡å‹
        
        Args:
            save_onnx: ä¿å­˜è·¯å¾„
        """
    
    def convert_opset_version(self, save_path: str, target_version: int):
        """
        è½¬æ¢ONNXæ¨¡å‹çš„Opsetç‰ˆæœ¬
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            target_version: ç›®æ ‡Opsetç‰ˆæœ¬
        """
    
    def fix_input_shape(self, save_path: str, input_shapes: list):
        """
        å›ºå®šONNXæ¨¡å‹çš„è¾“å…¥å°ºå¯¸
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            input_shapes: è¾“å…¥å½¢çŠ¶åˆ—è¡¨
        """
    
    def dynamic_input_shape(self, save_path: str, dynamic_dims: list):
        """
        è®¾ç½®ONNXæ¨¡å‹çš„è¾“å…¥ä¸ºåŠ¨æ€å°ºå¯¸
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            dynamic_dims: åŠ¨æ€ç»´åº¦åˆ—è¡¨
        """
    
    def test_performance(self, loop: int = 10, warmup: int = 3):
        """
        æµ‹è¯•æ¨ç†æ€§èƒ½
        
        Args:
            loop: æ­£å¼æµ‹è¯•å¾ªç¯æ¬¡æ•°
            warmup: é¢„çƒ­æ¬¡æ•°
        """
```

### TensorRT éƒ¨ç½²

```python
class TRTInfer:
    """TensorRTæ¨ç†ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–TensorRTæ¨ç†"""
    
    def load_model(self, engine_path: str):
        """
        åŠ è½½TensorRTå¼•æ“
        
        Args:
            engine_path: å¼•æ“æ–‡ä»¶è·¯å¾„
        """
    
    def build_engine(self, onnx_path: str, engine_path: str, max_workspace_size=4<<30, 
                    fp16=False, dynamic_shape_profile=None, hardware_compatibility="", 
                    optimization_level=3, version_compatible=False):
        """
        ä»ONNXæ„å»ºTensorRTå¼•æ“
        
        Args:
            onnx_path: ONNXæ¨¡å‹è·¯å¾„
            engine_path: è¾“å‡ºå¼•æ“è·¯å¾„
            max_workspace_size: æœ€å¤§å·¥ä½œç©ºé—´å¤§å°
            fp16: æ˜¯å¦ä½¿ç”¨FP16
            dynamic_shape_profile: åŠ¨æ€å½¢çŠ¶é…ç½®
            hardware_compatibility: ç¡¬ä»¶å…¼å®¹æ€§
            optimization_level: ä¼˜åŒ–çº§åˆ«
            version_compatible: ç‰ˆæœ¬å…¼å®¹æ€§
        """
    
    def __call__(self, data):
        """
        æ‰§è¡Œæ¨ç†
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            List[np.ndarray]: æ¨ç†ç»“æœ
        """
    
    def test_performance(self, loop: int = 10, warmup: int = 3) -> float:
        """
        æµ‹è¯•æ¨ç†æ€§èƒ½
        
        Args:
            loop: æ­£å¼æµ‹è¯•å¾ªç¯æ¬¡æ•°
            warmup: é¢„çƒ­æ¬¡æ•°
            
        Returns:
            float: å¹³å‡æ¨ç†æ—¶é—´
        """
```

### å…±äº«å†…å­˜éƒ¨ç½²

```python
class SharedMemoryServer:
    """å…±äº«å†…å­˜æœåŠ¡å™¨"""
    
    def __init__(self, name: str, model_path: str):
        """
        åˆå§‹åŒ–å…±äº«å†…å­˜æœåŠ¡å™¨
        
        Args:
            name: æœåŠ¡å™¨åç§°
            model_path: æ¨¡å‹è·¯å¾„
        """
    
    def start(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
    
    def stop(self):
        """åœæ­¢æœåŠ¡å™¨"""

class SharedMemoryClient:
    """å…±äº«å†…å­˜å®¢æˆ·ç«¯"""
    
    def __init__(self, server_name: str):
        """
        åˆå§‹åŒ–å…±äº«å†…å­˜å®¢æˆ·ç«¯
        
        Args:
            server_name: æœåŠ¡å™¨åç§°
        """
    
    def infer(self, input_data: dict):
        """
        é€šè¿‡å…±äº«å†…å­˜æ‰§è¡Œæ¨ç†
        
        Args:
            input_data: è¾“å…¥æ•°æ®å­—å…¸
            
        Returns:
            dict: æ¨ç†ç»“æœ
        """
```

## ğŸ“– ä½¿ç”¨æŒ‡å—

### 1. ONNX Runtime éƒ¨ç½²

#### åŸºæœ¬æ¨ç†

```python
from sindre.deploy import onnxruntime_deploy
import numpy as np

# åˆ›å»ºæ¨ç†å®ä¾‹
infer = onnxruntime_deploy.OnnxInfer("model.onnx")

# å‡†å¤‡è¾“å…¥æ•°æ®
input_data = np.random.rand(1, 3, 224, 224).astype(np.float32)

# æ‰§è¡Œæ¨ç†
result = infer(input_data)
print(f"æ¨ç†ç»“æœ: {result}")

# è·å–æ¨ç†æ—¶é—´
infer.test_performance(loop=10, warmup=3)
```

#### å¤šè¾“å…¥æ¨ç†

```python
# å¤šè¾“å…¥æ¨¡å‹
input_data = {
    "input1": np.random.rand(1, 3, 224, 224).astype(np.float32),
    "input2": np.random.rand(1, 10).astype(np.float32)
}

result = infer(input_data)
print(f"å¤šè¾“å…¥æ¨ç†ç»“æœ: {result}")
```

#### æ¨¡å‹ä¼˜åŒ–

```python
# ä¼˜åŒ–æ¨¡å‹
infer.optimizer("optimized_model.onnx")

# è½¬æ¢Opsetç‰ˆæœ¬
infer.convert_opset_version("model_v16.onnx", 16)

# å›ºå®šè¾“å…¥å½¢çŠ¶
infer.fix_input_shape("fixed_model.onnx", [[1, 3, 224, 224]])

# è®¾ç½®åŠ¨æ€è¾“å…¥
infer.dynamic_input_shape("dynamic_model.onnx", [[None, 3, None, None]])
```

### 2. TensorRT éƒ¨ç½²

#### åŸºæœ¬æ¨ç†

```python
from sindre.deploy import TenserRT_deploy
import numpy as np

# åˆ›å»ºæ¨ç†å®ä¾‹
trt_infer = TenserRT_deploy.TRTInfer()

# åŠ è½½å¼•æ“
trt_infer.load_model("model.engine")

# å‡†å¤‡è¾“å…¥æ•°æ®
input_data = np.random.rand(1, 3, 224, 224).astype(np.float32)

# æ‰§è¡Œæ¨ç†
result = trt_infer(input_data)
print(f"TensorRTæ¨ç†ç»“æœ: {result}")

# æ€§èƒ½æµ‹è¯•
avg_time = trt_infer.test_performance(loop=100, warmup=10)
print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.3f}ms")
```

#### æ„å»ºå¼•æ“

```python
# ä»ONNXæ„å»ºå¼•æ“
trt_infer.build_engine(
    onnx_path="model.onnx",
    engine_path="model.engine",
    max_workspace_size=4<<30,  # 4GB
    fp16=True,  # ä½¿ç”¨FP16
    optimization_level=3
)

# åŠ¨æ€å½¢çŠ¶å¼•æ“
dynamic_profile = {
    "input": [(1, 3, 224, 224), (4, 3, 224, 224), (8, 3, 224, 224)]
}

trt_infer.build_engine(
    onnx_path="model.onnx",
    engine_path="dynamic_model.engine",
    dynamic_shape_profile=dynamic_profile
)
```

### 3. å…±äº«å†…å­˜éƒ¨ç½²

#### æœåŠ¡å™¨ç«¯

```python
from sindre.deploy import python_share_memory

# åˆ›å»ºæœåŠ¡å™¨
server = python_share_memory.SharedMemoryServer("model_server", "model.onnx")

# å¯åŠ¨æœåŠ¡å™¨
server.start()

# ä¿æŒè¿è¡Œ
try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    server.stop()
```

#### å®¢æˆ·ç«¯

```python
from sindre.deploy import python_share_memory

# åˆ›å»ºå®¢æˆ·ç«¯
client = python_share_memory.SharedMemoryClient("model_server")

# å‡†å¤‡æ•°æ®
input_data = {
    "input": np.random.rand(1, 3, 224, 224).astype(np.float32)
}

# æ‰§è¡Œæ¨ç†
result = client.infer(input_data)
print(f"å…±äº«å†…å­˜æ¨ç†ç»“æœ: {result}")
```

### 4. ç³»ç»Ÿæ£€æµ‹å·¥å…·

```python
from sindre.deploy import check_tools

# æ£€æµ‹GPUå’Œç³»ç»Ÿä¿¡æ¯
check_tools.check_gpu_info()

# æ€§èƒ½æµ‹é‡å·¥å…· - CPUæ¨¡å¼
with check_tools.timeit("CPUè®¡ç®—"):
    result = [i**2 for i in range(10**6)]

# æ€§èƒ½æµ‹é‡å·¥å…· - GPUæ¨¡å¼
import torch
if torch.cuda.is_available():
    with check_tools.timeit("GPUè®¡ç®—", use_torch=True):
        tensor = torch.randn(10000, 10000).cuda()
        result = tensor @ tensor.T
```

**check_gpu_info() åŠŸèƒ½**:
- æ£€æµ‹æ“ä½œç³»ç»Ÿä¿¡æ¯
- æ˜¾ç¤ºCPUæ ¸å¿ƒæ•°ã€é¢‘ç‡ã€ä½¿ç”¨ç‡
- æ˜¾ç¤ºå†…å­˜æ€»é‡å’Œä½¿ç”¨æƒ…å†µ
- æ£€æµ‹GPUè®¾å¤‡æ•°é‡å’Œè¯¦ç»†ä¿¡æ¯
- æ˜¾ç¤ºCUDAå’ŒcuDNNç‰ˆæœ¬
- æ£€æŸ¥ç¡¬ä»¶æ”¯æŒçš„æ•°æ®ç±»å‹ï¼ˆFP16ã€BF16ã€INT8ç­‰ï¼‰

**timeit ä¸Šä¸‹æ–‡ç®¡ç†å™¨**:
- æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´
- ç›‘æ§å†…å­˜ä½¿ç”¨å˜åŒ–
- æ”¯æŒCPUå’ŒGPUæ¨¡å¼
- æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆGPUæ¨¡å¼ï¼‰

## ğŸš€ é«˜çº§åŠŸèƒ½

### 1. æ¨¡å‹ä¼˜åŒ–

#### ONNXä¼˜åŒ–

```python
# è‡ªåŠ¨ä¼˜åŒ–
infer.optimizer("optimized.onnx")

# æ‰‹åŠ¨ä¼˜åŒ–é€‰é¡¹
optimization_passes = [
    'eliminate_deadend',
    'eliminate_identity',
    'fuse_bn_into_conv',
    'fuse_consecutive_concats'
]
```

#### TensorRTä¼˜åŒ–

```python
# FP16ä¼˜åŒ–
trt_infer.build_engine(
    onnx_path="model.onnx",
    engine_path="fp16_model.engine",
    fp16=True
)

# INT8é‡åŒ–
trt_infer.build_engine(
    onnx_path="model.onnx",
    engine_path="int8_model.engine",
    int8=True
)
```

### 2. åŠ¨æ€å½¢çŠ¶æ”¯æŒ

```python
# ONNXåŠ¨æ€å½¢çŠ¶
infer.dynamic_input_shape("dynamic.onnx", [[None, 3, None, None]])

# TensorRTåŠ¨æ€å½¢çŠ¶
dynamic_profile = {
    "input": [(1, 3, 224, 224), (4, 3, 224, 224), (8, 3, 224, 224)]
}
trt_infer.build_engine(
    onnx_path="model.onnx",
    engine_path="dynamic.engine",
    dynamic_shape_profile=dynamic_profile
)
```

### 3. æ€§èƒ½ç›‘æ§

```python
# ONNXæ€§èƒ½æµ‹è¯•
infer.test_performance(loop=100, warmup=10)

# TensorRTæ€§èƒ½æµ‹è¯•
avg_time = trt_infer.test_performance(loop=100, warmup=10)
print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.3f}ms")

# å†…å­˜ä½¿ç”¨ç›‘æ§
import psutil
process = psutil.Process()
memory_usage = process.memory_info().rss / 1024 / 1024  # MB
print(f"å†…å­˜ä½¿ç”¨: {memory_usage:.2f} MB")
```

### 4. é”™è¯¯å¤„ç†

```python
try:
    result = infer(input_data)
except RuntimeError as e:
    print(f"æ¨ç†é”™è¯¯: {e}")
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists("model.onnx"):
        print("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    if input_data.shape != expected_shape:
        print(f"è¾“å…¥å½¢çŠ¶ä¸åŒ¹é…: {input_data.shape} vs {expected_shape}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–

```python
# ä½¿ç”¨å†…å­˜æ± 
import numpy as np
from contextlib import contextmanager

@contextmanager
def memory_pool():
    """å†…å­˜æ± ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    pool = {}
    try:
        yield pool
    finally:
        pool.clear()

# ä½¿ç”¨å†…å­˜æ± 
with memory_pool() as pool:
    if "input_buffer" not in pool:
        pool["input_buffer"] = np.zeros((1, 3, 224, 224), dtype=np.float32)
    input_data = pool["input_buffer"]
    result = infer(input_data)
```

### 2. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# æ‰¹é‡æ¨ç†
def batch_inference(infer, data_list, batch_size=4):
    """æ‰¹é‡æ¨ç†"""
    results = []
    for i in range(0, len(data_list), batch_size):
        batch = data_list[i:i+batch_size]
        batch_result = infer(batch)
        results.extend(batch_result)
    return results

# ä½¿ç”¨æ‰¹é‡æ¨ç†
data_list = [np.random.rand(1, 3, 224, 224) for _ in range(100)]
results = batch_inference(infer, data_list, batch_size=8)
```

### 3. å¤šçº¿ç¨‹ä¼˜åŒ–

```python
import threading
from queue import Queue

class ThreadedInfer:
    """å¤šçº¿ç¨‹æ¨ç†"""
    def __init__(self, model_path, num_threads=4):
        self.infer = onnxruntime_deploy.OnnxInfer(model_path)
        self.queue = Queue()
        self.threads = []
        
        for _ in range(num_threads):
            thread = threading.Thread(target=self._worker)
            thread.start()
            self.threads.append(thread)
    
    def _worker(self):
        while True:
            try:
                data, callback = self.queue.get(timeout=1)
                result = self.infer(data)
                callback(result)
            except:
                break
    
    def infer_async(self, data, callback):
        """å¼‚æ­¥æ¨ç†"""
        self.queue.put((data, callback))

# ä½¿ç”¨å¤šçº¿ç¨‹æ¨ç†
threaded_infer = ThreadedInfer("model.onnx", num_threads=4)

def on_result(result):
    print(f"å¼‚æ­¥æ¨ç†ç»“æœ: {result}")

threaded_infer.infer_async(input_data, on_result)
```

## â“ å¸¸è§é—®é¢˜

### Q1: ONNXæ¨¡å‹åŠ è½½å¤±è´¥ï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
```python
# 1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
if not os.path.exists("model.onnx"):
    print("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")

# 2. æ£€æŸ¥æ¨¡å‹æ ¼å¼
import onnx
try:
    model = onnx.load("model.onnx")
    onnx.checker.check_model(model)
except Exception as e:
    print(f"æ¨¡å‹æ ¼å¼é”™è¯¯: {e}")

# 3. æ£€æŸ¥æ¨ç†æä¾›è€…
available_providers = onnxruntime.get_available_providers()
print(f"å¯ç”¨æä¾›è€…: {available_providers}")
```

### Q2: TensorRTå¼•æ“æ„å»ºå¤±è´¥ï¼Ÿ

**A**: å¸¸è§è§£å†³æ–¹æ¡ˆï¼š
```python
# 1. æ£€æŸ¥TensorRTç‰ˆæœ¬
import tensorrt as trt
print(f"TensorRTç‰ˆæœ¬: {trt.__version__}")

# 2. æ£€æŸ¥CUDAç‰ˆæœ¬
import torch
print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")

# 3. å‡å°‘å·¥ä½œç©ºé—´å¤§å°
trt_infer.build_engine(
    onnx_path="model.onnx",
    engine_path="model.engine",
    max_workspace_size=1<<30  # 1GB
)
```

### Q3: æ¨ç†æ€§èƒ½ä¸ç†æƒ³ï¼Ÿ

**A**: æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š
```python
# 1. ä½¿ç”¨GPUæ¨ç†
infer = onnxruntime_deploy.OnnxInfer(
    "model.onnx",
    providers=[('CUDAExecutionProvider', {})]
)

# 2. å¯ç”¨æ¨¡å‹ä¼˜åŒ–
infer.optimizer("optimized.onnx")

# 3. ä½¿ç”¨FP16
trt_infer.build_engine(
    onnx_path="model.onnx",
    engine_path="fp16.engine",
    fp16=True
)
```

### Q4: å†…å­˜ä¸è¶³ï¼Ÿ

**A**: å†…å­˜ä¼˜åŒ–æ–¹æ³•ï¼š
```python
# 1. å‡å°‘æ‰¹å¤„ç†å¤§å°
batch_size = 1  # ä»4å‡å°‘åˆ°1

# 2. ä½¿ç”¨åŠ¨æ€å½¢çŠ¶
infer.dynamic_input_shape("dynamic.onnx", [[None, 3, None, None]])

# 3. åŠæ—¶é‡Šæ”¾å†…å­˜
import gc
gc.collect()
```

### Q5: å…±äº«å†…å­˜è¿æ¥å¤±è´¥ï¼Ÿ

**A**: æ£€æŸ¥è¿æ¥è®¾ç½®ï¼š
```python
# 1. æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
if not server.is_running():
    print("æœåŠ¡å™¨æœªè¿è¡Œ")

# 2. æ£€æŸ¥ç«¯å£å ç”¨
import socket
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
result = sock.connect_ex(('localhost', 8080))
if result == 0:
    print("ç«¯å£è¢«å ç”¨")

# 3. æ£€æŸ¥æƒé™
import os
if not os.access("/dev/shm", os.W_OK):
    print("å…±äº«å†…å­˜æƒé™ä¸è¶³")
```

### Q6: æ¨¡å‹ç²¾åº¦ä¸‹é™ï¼Ÿ

**A**: ç²¾åº¦ä¼˜åŒ–æ–¹æ³•ï¼š
```python
# 1. ä½¿ç”¨FP32è€Œä¸æ˜¯FP16
trt_infer.build_engine(
    onnx_path="model.onnx",
    engine_path="fp32.engine",
    fp16=False
)

# 2. æ£€æŸ¥é‡åŒ–è®¾ç½®
# é¿å…è¿‡åº¦é‡åŒ–

# 3. éªŒè¯æ¨ç†ç»“æœ
expected_result = reference_inference(input_data)
actual_result = optimized_inference(input_data)
diff = np.abs(expected_result - actual_result).max()
print(f"æœ€å¤§è¯¯å·®: {diff}")
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

| æ¨ç†å¼•æ“ | æ¨¡å‹å¤§å° | æ¨ç†æ—¶é—´ | å†…å­˜ä½¿ç”¨ | é€‚ç”¨åœºæ™¯ |
|----------|----------|----------|----------|----------|
| ONNX Runtime (CPU) | 50MB | ~50ms | ~200MB | å¼€å‘æµ‹è¯• |
| ONNX Runtime (GPU) | 50MB | ~10ms | ~500MB | ç”Ÿäº§ç¯å¢ƒ |
| TensorRT | 50MB | ~5ms | ~800MB | é«˜æ€§èƒ½éœ€æ±‚ |
| å…±äº«å†…å­˜ | 50MB | ~2ms | ~100MB | ä½å»¶è¿Ÿéœ€æ±‚ |

## ğŸ”— ç›¸å…³é“¾æ¥

- [ONNX Runtimeæ–‡æ¡£](https://onnxruntime.ai/)
- [TensorRTæ–‡æ¡£](https://docs.nvidia.com/deeplearning/tensorrt/)
- [OpenVINOæ–‡æ¡£](https://docs.openvino.ai/)
- [å…±äº«å†…å­˜æ–‡æ¡£](https://docs.python.org/3/library/multiprocessing.html#shared-memory)

---



å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜) æˆ–æäº¤ [Issue](https://github.com/SindreYang/sindre/issues)
