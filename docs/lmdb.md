# LMDB æ•°æ®åº“æ¨¡å—

> é«˜æ€§èƒ½çš„LMDBæ•°æ®åº“æ“ä½œæ¨¡å—ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®å­˜å‚¨å’Œé«˜æ•ˆè¯»å–

## ğŸ“‹ ç›®å½•

- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ ¸å¿ƒç±»](#æ ¸å¿ƒç±»)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½**: åŸºäºLMDBçš„é«˜æ€§èƒ½æ•°æ®åº“æ“ä½œ
- ğŸ“¦ **æ•°æ®åºåˆ—åŒ–**: è‡ªåŠ¨å¤„ç†numpyæ•°ç»„å’Œå¤æ‚æ•°æ®ç»“æ„
- ğŸ”„ **å¤šè¿›ç¨‹æ”¯æŒ**: æ”¯æŒå¤šè¿›ç¨‹å¹¶å‘è¯»å†™
- ğŸ’¾ **å†…å­˜ä¼˜åŒ–**: æ™ºèƒ½å†…å­˜ç®¡ç†å’Œå¤§å°æ§åˆ¶
- ğŸ”§ **å·¥å…·ä¸°å¯Œ**: æä¾›æ•°æ®åº“åˆå¹¶ã€åˆ†å‰²ã€ä¿®å¤ç­‰å·¥å…·
- ğŸ›¡ï¸ **æ•°æ®å®‰å…¨**: äº‹åŠ¡æ€§æ“ä½œï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
import sindre.lmdb as lmdb
import numpy as np

# åˆ›å»ºæ•°æ®åº“,æ”¯æŒç›®å½•ï¼Œä¹Ÿæ”¯æŒæ–‡ä»¶
writer = lmdb.Writer('./data.db', map_size_limit=1024*100)  # map_size_limitå•ä½ä¸ºMB 
#writer = lmdb.Writer('./data', map_size_limit=1024*100)  # ä¼šåˆ›å»ºdataç›®å½•

# å†™å…¥æ•°æ®
data = {
    'points': np.random.rand(100, 3),
    'labels': np.random.randint(0, 10, 100),
    'version': '1.0'ï¼Œ
}
writer.put_samples(data)
writer.close()

# è¯»å–æ•°æ®
reader = lmdb.Reader('./data.db')
sample = reader[0]
print(f"è¯»å–åˆ° {len(sample['points'])} ä¸ªç‚¹")
reader.close()
```

### PyTorch æ•°æ®é›†é›†æˆ

```python
import torch
from sindre.lmdb import Reader

class LMDBDataset(torch.utils.data.Dataset):
    def __init__(self, db_path):
        self.db = Reader(db_path, multiprocessing=False)
    
    def __len__(self):
        return len(self.db)
    
    def __getitem__(self, idx):
        data = self.db[idx]
        # è½¬æ¢ä¸ºtorchå¼ é‡
        return {k: torch.from_numpy(v) for k, v in data.items()}

# ä½¿ç”¨
dataset = LMDBDataset('./data')
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)
```

## ğŸ”§ æ ¸å¿ƒç±»

### Writer - æ•°æ®åº“å†™å…¥å™¨

```python
class Writer:
    """LMDBæ•°æ®åº“å†™å…¥å™¨"""
    
    def __init__(self, dirpath: str, map_size_limit: int, multiprocessing: bool = False):
        """
        åˆå§‹åŒ–å†™å…¥å™¨
        
        Args:
            dirpath: æ•°æ®åº“ç›®å½•è·¯å¾„
            map_size_limit: æ•°æ®åº“å¤§å°é™åˆ¶ï¼ˆMBï¼‰
            multiprocessing: æ˜¯å¦å¯ç”¨å¤šè¿›ç¨‹æ”¯æŒ
        """
    
    def put_samples(self, samples: dict):
        """æ‰¹é‡å†™å…¥æ ·æœ¬æ•°æ®"""
    
    def change_value(self, num_id: int, samples: dict):
        """ä¿®æ”¹æŒ‡å®šIDçš„æ•°æ®"""
    
    def change_db_value(self, key: int, value: dict, safe_model: bool = True):
        """å®‰å…¨ä¿®æ”¹æ•°æ®åº“å€¼ï¼Œå¸¦ç¡®è®¤æç¤º"""
    
    def check_sample_size(self, samples: dict):
        """æ£€æŸ¥æ ·æœ¬å¤§å°ï¼ˆGBï¼‰"""
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
```

### Reader - æ•°æ®åº“è¯»å–å™¨

```python
class Reader:
    """LMDBæ•°æ®åº“è¯»å–å™¨"""
    
    def __init__(self, dirpath: str, multiprocessing: bool = False):
        """
        åˆå§‹åŒ–è¯»å–å™¨
        
        Args:
            dirpath: æ•°æ®åº“ç›®å½•è·¯å¾„
            multiprocessing: æ˜¯å¦å¯ç”¨å¤šè¿›ç¨‹æ”¯æŒ
        """
    
    def __getitem__(self, idx: int):
        """é€šè¿‡ç´¢å¼•è·å–æ•°æ®"""
    
    def get_sample(self, idx: int):
        """è·å–å•ä¸ªæ ·æœ¬"""
    
    def get_samples(self, start_idx: int, size: int):
        """æ‰¹é‡è·å–æ ·æœ¬"""
    
    def get_data_keys(self, i: int = 0):
        """è·å–ç¬¬iä¸ªæ ·æœ¬çš„æ‰€æœ‰é”®"""
    
    def get_data_value(self, i: int, key: str):
        """è·å–ç¬¬iä¸ªæ ·æœ¬çš„æŒ‡å®šé”®å€¼"""
    
    def get_data_specification(self, i: int):
        """è·å–ç¬¬iä¸ªæ ·æœ¬çš„æ•°æ®è§„èŒƒ"""
    
    def get_meta_str(self, key):
        """è·å–å…ƒæ•°æ®å­—ç¬¦ä¸²"""
    
    def __len__(self):
        """è·å–æ•°æ®åº“å¤§å°"""
```

### ReaderList - å¤šæ•°æ®åº“è¯»å–å™¨

```python
class ReaderList:
    """å¤šä¸ªLMDBæ•°æ®åº“çš„ç»Ÿä¸€è¯»å–å™¨"""
    
    def __init__(self, db_path_list: list, multiprocessing: bool = True):
        """
        åˆå§‹åŒ–å¤šæ•°æ®åº“è¯»å–å™¨
        
        Args:
            db_path_list: æ•°æ®åº“è·¯å¾„åˆ—è¡¨
            multiprocessing: æ˜¯å¦å¯ç”¨å¤šè¿›ç¨‹æ”¯æŒ
        """
```

### ReaderSSD - SSDä¼˜åŒ–è¯»å–å™¨

```python
class ReaderSSD:
    """é’ˆå¯¹SSDä¼˜åŒ–çš„è¯»å–å™¨"""
    
    def __init__(self, db_path: str, multiprocessing: bool = False):
        """
        åˆå§‹åŒ–SSDè¯»å–å™¨
        
        Args:
            db_path: æ•°æ®åº“è·¯å¾„
            multiprocessing: æ˜¯å¦å¯ç”¨å¤šè¿›ç¨‹æ”¯æŒ
        """
    
    def get_batch(self, indices: list):
        """æ‰¹é‡è·å–æ•°æ®"""
```

## ğŸ“– ä½¿ç”¨æŒ‡å—

### 1. æ•°æ®å†™å…¥

#### åŸºæœ¬å†™å…¥

```python
import sindre.lmdb as lmdb
import numpy as np

# åˆ›å»ºå†™å…¥å™¨
writer = lmdb.Writer('./dataset', map_size_limit=1024*100)  # 100GB

# å†™å…¥å•ä¸ªæ ·æœ¬
data = {
    'points': np.random.rand(1000, 3),
    'labels': np.random.randint(0, 10, 1000),
    'features': np.random.rand(1000, 128)
}
writer.put_samples({0: data})

# æ‰¹é‡å†™å…¥
for i in range(1000):
    data = {
        'points': np.random.rand(100, 3),
        'labels': np.random.randint(0, 10, 100),
        'id': i
    }
    writer.put_samples({i: data})

writer.close()
```

#### è®¾ç½®å…ƒæ•°æ®

```python
# è®¾ç½®æ•°æ®åº“å…ƒæ•°æ®
writer.set_meta_str("description", "ç‚¹äº‘æ•°æ®é›†")
writer.set_meta_str("version", "1.0")
writer.set_meta_str("created_by", "sindre")
```

#### æ•°æ®ä¿®æ”¹

```python
# ä¿®æ”¹ç°æœ‰æ•°æ®
new_data = {
    'points': np.random.rand(200, 3),
    'labels': np.random.randint(0, 10, 200),
    'updated': True
}
writer.change_value(0, new_data)

# å®‰å…¨ä¿®æ”¹ï¼ˆå¸¦ç¡®è®¤æç¤ºï¼‰
writer.change_db_value(0, new_data, safe_model=True)
```

#### å†…å­˜å¤§å°æ£€æŸ¥

```python
# æ£€æŸ¥æ•°æ®å¤§å°
data = {
    'points': np.random.rand(10000, 3),
    'labels': np.random.randint(0, 10, 10000)
}
gb_required = writer.check_sample_size(data)
print(f"æ•°æ®å¤§å°: {gb_required:.2f} GB")
```

### 2. æ•°æ®è¯»å–

#### åŸºæœ¬è¯»å–

```python
# åˆ›å»ºè¯»å–å™¨
reader = lmdb.Reader('./dataset')

# è·å–æ•°æ®åº“å¤§å°
print(f"æ•°æ®åº“åŒ…å« {len(reader)} ä¸ªæ ·æœ¬")

# è¯»å–å•ä¸ªæ ·æœ¬
sample = reader[0]
print(f"æ ·æœ¬é”®: {list(sample.keys())}")

# è¯»å–æŒ‡å®šæ ·æœ¬
sample = reader.get_sample(0)
print(f"ç‚¹äº‘æ•°é‡: {len(sample['points'])}")
```

#### æ‰¹é‡è¯»å–

```python
# æ‰¹é‡è¯»å–
samples = reader.get_samples(0, 10)
print(f"è¯»å–äº† {len(samples)} ä¸ªæ ·æœ¬")

# ä½¿ç”¨ReaderListè¯»å–å¤šä¸ªæ•°æ®åº“
reader_list = lmdb.ReaderList(['./db1', './db2', './db3'])
print(f"æ€»æ ·æœ¬æ•°: {len(reader_list)}")
```

#### å…ƒæ•°æ®æŸ¥è¯¢

```python
# è·å–å…ƒæ•°æ®
description = reader.get_meta_str("description")
version = reader.get_meta_str("version")
print(f"æè¿°: {description}, ç‰ˆæœ¬: {version}")

# è·å–æ•°æ®é”®ä¿¡æ¯
data_keys = reader.get_data_keys(0)
print(f"æ•°æ®é”®: {data_keys}")

# è·å–æ•°æ®è§„èŒƒ
spec = reader.get_data_specification(0)
for key, info in spec.items():
    print(f"{key}: shape={info['shape']}, dtype={info['dtype']}")
```

### 3. å¤šè¿›ç¨‹æ”¯æŒ

```python
# å¯ç”¨å¤šè¿›ç¨‹å†™å…¥
writer = lmdb.Writer('./dataset', map_size_limit=1024*100, multiprocessing=True)

# å¯ç”¨å¤šè¿›ç¨‹è¯»å–
reader = lmdb.Reader('./dataset', multiprocessing=True)
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### æ•°æ®åº“å·¥å…·å‡½æ•°

```python
import sindre.lmdb as lmdb

# åˆå¹¶æ•°æ®åº“
lmdb.MergeLmdb(
    target_dir='./merged_db',
    source_dirs=['./db1', './db2', './db3'],
    map_size_limit=1024*100,
    multiprocessing=True
)

# åˆ†å‰²æ•°æ®åº“
lmdb.SplitLmdb(
    source_dir='./large_db',
    target_dirs=['./part1', './part2', './part3'],
    map_size_limit=1024*50,
    multiprocessing=True
)

# ä¿®å¤Windowså¤§å°é—®é¢˜
lmdb.fix_lmdb_windows_size('./database')

# å¹¶è¡Œå†™å…¥
def process_function(file_path):
    # å¤„ç†å•ä¸ªæ–‡ä»¶çš„å‡½æ•°
    return {'processed_data': np.random.rand(100, 3)}

lmdb.parallel_write(
    output_dir='./processed_db',
    file_list=['file1.txt', 'file2.txt', 'file3.txt'],
    process=process_function,
    map_size_limit=1024*100,
    num_processes=4,
    multiprocessing=True
)
```

### SSDä¼˜åŒ–è¯»å–

```python
# ä½¿ç”¨SSDä¼˜åŒ–è¯»å–å™¨
reader_ssd = lmdb.ReaderSSD('./dataset', multiprocessing=False)

# æ‰¹é‡è¯»å–
indices = [0, 1, 2, 3, 4]
batch_data = reader_ssd.get_batch(indices)
print(f"æ‰¹é‡è¯»å–äº† {len(batch_data)} ä¸ªæ ·æœ¬")

# å¤šæ•°æ®åº“SSDè¯»å–
reader_ssd_list = lmdb.ReaderSSDList(['./db1', './db2'], multiprocessing=False)
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ç®¡ç†

```python
# åˆç†è®¾ç½®map_size_limit
# å»ºè®®è®¾ç½®ä¸ºé¢„æœŸæ•°æ®å¤§å°çš„1.5-2å€
expected_size_gb = 50
map_size_limit_mb = int(expected_size_gb * 1.5 * 1024)
writer = lmdb.Writer('./dataset', map_size_limit=map_size_limit_mb)
```

### 2. å¤šè¿›ç¨‹ä¼˜åŒ–

```python
# å†™å…¥æ—¶ä½¿ç”¨å¤šè¿›ç¨‹
writer = lmdb.Writer('./dataset', map_size_limit=1024*100, multiprocessing=True)

# è¯»å–æ—¶æ ¹æ®æ•°æ®å¤§å°å†³å®šæ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹
if len(reader) > 10000:
    reader = lmdb.Reader('./dataset', multiprocessing=True)
else:
    reader = lmdb.Reader('./dataset', multiprocessing=False)
```

### 3. æ‰¹é‡æ“ä½œ

```python
# æ‰¹é‡å†™å…¥è€Œä¸æ˜¯é€ä¸ªå†™å…¥
batch_data = {}
for i in range(1000):
    batch_data[i] = {
        'points': np.random.rand(100, 3),
        'labels': np.random.randint(0, 10, 100)
    }
writer.put_samples(batch_data)
```

## â“ å¸¸è§é—®é¢˜

### Q1: map_size_limit è®¾ç½®å¤šå¤§åˆé€‚ï¼Ÿ

**A**: å»ºè®®è®¾ç½®ä¸ºé¢„æœŸæ•°æ®å¤§å°çš„1.5-2å€ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ•°æ®å¤§çº¦50GBï¼Œå¯ä»¥è®¾ç½®ä¸ºï¼š
```python
map_size_limit = int(50 * 1.5 * 1024)  # 75GB in MB
```

### Q2: å¤šè¿›ç¨‹æ¨¡å¼ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ï¼Ÿ

**A**: 
- **å†™å…¥æ—¶**: æ•°æ®é‡å¤§ï¼ˆ>1GBï¼‰æ—¶å»ºè®®ä½¿ç”¨
- **è¯»å–æ—¶**: æ•°æ®åº“æ ·æœ¬æ•°å¤šï¼ˆ>10000ï¼‰æ—¶å»ºè®®ä½¿ç”¨

### Q3: å¦‚ä½•å¤„ç†æ•°æ®åº“æŸåï¼Ÿ

**A**: ä½¿ç”¨ä¿®å¤å·¥å…·ï¼š
```python
lmdb.fix_lmdb_windows_size('./database')
```

### Q4: å¦‚ä½•æ£€æŸ¥æ•°æ®åº“çŠ¶æ€ï¼Ÿ

**A**: 
```python
writer.check_db_stats()  # æ£€æŸ¥æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
```

### Q5: æ”¯æŒå“ªäº›æ•°æ®ç±»å‹ï¼Ÿ

**A**: ä¸»è¦æ”¯æŒnumpyæ•°ç»„ï¼Œå…¶ä»–ç±»å‹ä¼šè‡ªåŠ¨è½¬æ¢ï¼š
```python
# æ”¯æŒçš„æ•°æ®
data = {
    'points': np.random.rand(100, 3),      # numpyæ•°ç»„
    'labels': np.random.randint(0, 10, 100), # numpyæ•°ç»„
    'metadata': 'test'                      # å­—ç¬¦ä¸²ï¼ˆä¼šè¢«åºåˆ—åŒ–ï¼‰
}
```

### Q6: å¦‚ä½•é«˜æ•ˆåœ°ä¿®æ”¹ç°æœ‰æ•°æ®ï¼Ÿ

**A**: ä½¿ç”¨å®‰å…¨ä¿®æ”¹æ¨¡å¼ï¼š
```python
# å¸¦ç¡®è®¤æç¤ºçš„å®‰å…¨ä¿®æ”¹
writer.change_db_value(0, new_data, safe_model=True)

# ç›´æ¥ä¿®æ”¹ï¼ˆæ— ç¡®è®¤ï¼‰
writer.change_value(0, new_data)
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

| æ“ä½œ | æ•°æ®å¤§å° | æ—¶é—´ | å†…å­˜ä½¿ç”¨ |
|------|----------|------|----------|
| å†™å…¥ | 1GB | ~30s | ~2GB |
| è¯»å– | 1GB | ~5s | ~1GB |
| æ‰¹é‡è¯»å– | 1GB | ~2s | ~1.5GB |
| éšæœºè®¿é—® | 1GB | ~10s | ~1GB |

## ğŸ”— ç›¸å…³é“¾æ¥

- [LMDBå®˜æ–¹æ–‡æ¡£](https://lmdb.readthedocs.io/)
- [PyTorchæ•°æ®é›†æ•™ç¨‹](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)
- [NumPyæ–‡æ¡£](https://numpy.org/doc/)

---

<div align="center">

**å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜) æˆ–æäº¤ [Issue](https://github.com/SindreYang/sindre/issues)**

</div>

   
